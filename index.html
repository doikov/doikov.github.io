<html>
<head>
    <meta charset="UTF-8"/>
    <meta name="author" content="Nikita Doikov">
    <meta name="description" content="a homepage">

    <meta property="og:site_name" content="Nikita Doikov">
    <meta property="og:url" content="https://doikov.com/">
    <meta property="og:title" content="Nikita Doikov">
    <meta property="og:description" content="a homepage">
    <meta property="og:image" content="https://doikov.com/photos/me192.png">

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-170271039-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-170271039-1');
    </script>
<title>
    Nikita Doikov
</title>
    
<link rel="stylesheet" type="text/css" 
    href="https://fonts.googleapis.com/css?family=Open+Sans:400,600" />
<style type="text/css">

.page, a, h1, h3 {
    font-family: "Open Sans", sans-serif;
}

body {
    background-color: #242733;
    background-image: url("background.jpg");
    background-size: cover;
    background-repeat: no-repeat;
    margin: 0;
}

a {
    color: #9c0000;
    text-decoration: underline;
    pointer-events: auto;
}

a:focus,
a:hover {
    color: #EE0000;
    text-decoration: none;
}

.page {
    padding: 20px;
}

.sideblock {
    width: 270px;
    float: left;
    margin: 10px 20px 20px 10px;
    border: red 2px;
    background-color: #FFFFFF;
    font-family: "Open Sans";
}  

.centering {
    text-align: center;
}

.email {
    padding: 0 0 20px 20px;
}

.main {
    width: 60%;
    float: left;
    margin: 10px 10px 300px 10px;
    padding: 20px 30px 20px 30px;
    background-color: #FFFFFF;
}

.dark {
    color: #000000;
}

.location {
    font-style: italic;
}

.footer {
    color: gray;
    font-size: 12px;
    font-style: italic;
}

.hover_img a { 
    color: #336699;
    position: relative; 
    border-bottom: 1px dotted; 
    text-decoration: none;
}

.hover_img a span { 
    position: absolute; 
    display: none; 
    z-index: 99; 
}

.hover_img a:hover {
    color: #0000EE;
}

.hover_img a:hover span { 
    display: block; 
}

.main li { padding-bottom: 17px; }

.main h4, .main h3 { color: #012042; }

.main ul { margin-bottom: -10px; }

.bonus {
    color: green;
    font-weight: bold;
}

</style>
</head>

<body>
<div class="page">
    <div class="sideblock">
        <img src="photos/me.jpg" alt="Me" height="270" width="270" />
        <div class="centering">
            <h1>Nikita Doikov</h1>
            <h3>a homepage</h3>
        </div>
            <ul>

                <li><a href="CV.pdf">Curriculum Vitae</a></li>

                <li><a href="https://scholar.google.com/citations?user=YNBhhjUAAAAJ&hl=en">Google Scholar</a></li>

                <li><a href="https://github.com/doikov">GitHub</a></li>

            </ul>

        <div class="email">
            <tt>Email: <br/>nikita.doikov at epfl.ch</tt>
        </div>
        
    </div>
    <div class="main">
        
        <h4>Welcome!</h4>

        I do research in Computational Mathematics with a focus
        on Numerical Optimization and Machine Learning.

        <br/>
        <br/>

        Now I am a postdoctoral researcher at <a href="https://www.epfl.ch/">EPFL</a>, Switzerland,
        working in the <a href="https://www.epfl.ch/labs/mlo/">Machine Learning and Optimization Laboratory</a>
        with <a href="https://people.epfl.ch/martin.jaggi">Martin Jaggi</a>.

        <br/>
        <br/>
        <table cellpadding="0" style="width:100%;">
        <tr><td>
                I am excited to explore provably efficient optimization algorithms
        that exploit the problem structure and combine ideas from various fields.
        One of my areas of expertise is <i>second-order methods</i> and its <i>global complexity bounds</i>. I believe that bridging the gap between the second-order optimization theory
        and the best known computational practices is what
        will lead us to new achievements in the training process of our models.
        <br/>
        <br/>
        </td>
        <td style="vertical-align: top; padding-left: 5px;">
        <img src="trajectory.png" alt="trajectory" height="120" width="160" />
        </td></tr>
        </table>

        
        <table cellpadding="0" style="width:100%;">
        <tr><td>
        More broadly, I am interested in pursuing the following areas:
        <ul>
            <li>Optimization theory and algorithmic foundations of AI</li>
            <li>Convex and non-convex problem classes, complexity bounds</li>
            <li>Applications in statistics, machine learning, and scientific computing</li>
            <li>Scalable, distributed, and decentralized optimization</li>
        </ul>
        <br/>
        </td>
        <td style="vertical-align: top; text-align: right;">
        <img src="nonconvex.png" alt="non-convex" height="172" width="180" />
        </td></tr>
        </table>


        
        I defended my PhD in 2021 at <a href="https://uclouvain.be/en/index.html">UCLouvain</a>, Belgium, supervised by 
        <a href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>.
        My thesis is "<a href="thesis.pdf">New second-order and tensor methods
        in Convex Optimization</a>".
        
        <br/>
        <br/>
        I received a BSc degree in Computational Mathematics and Cybernetics
        from <a href="https://www.msu.ru/en/">Lomonosov Moscow State University</a> in 2015.
        I obtained a MSc degree 
        from <a href="https://www.hse.ru/en/">Higher School of Economics</a> in 2017, where I was studying advanced statistical and machine learning methods.
        <br/>
        <br/>
        <hr/>
        <h3>Papers</h3>
        <h4>Recent preprints / various:</h4>
        <ul>
            <li>
            <i>Improving Stochastic Cubic Newton with Momentum.</i><br/>
            <a class="dark" href="https://people.epfl.ch/el-mahdi.chayti?lang=en">El Mahdi Chayti</a>, 
            <strong>Nikita Doikov</strong>, and
            <a class="dark" href="https://people.epfl.ch/martin.jaggi">Martin Jaggi</a>, 2024
            (<a href="http://arxiv.org/abs/2410.19644">arXiv</a>)
            </li>
            <li>
            <i>Cubic regularized subspace Newton for non-convex optimization.</i><br/>
            <a class="dark" href="https://scholar.google.com/citations?user=GAxXyUUAAAAJ">Jim Zhao</a>,
            <a class="dark" href="https://scholar.google.com/citations?user=V1ONSgIAAAAJ">Aurelien Lucchi</a>, and
            <strong>Nikita Doikov</strong>, 2024
            (<a href="https://arxiv.org/abs/2406.16666">arXiv</a>)</li>
            <li>
            <i>Complexity of Minimizing Regularized Convex Quadratic Functions.</i><br/>
            <a class="dark" href="https://people.epfl.ch/331375">Daniel Berg Thomsen</a> and
            <strong>Nikita Doikov</strong>, 2024
            (<a href="https://arxiv.org/abs/2404.17543">arXiv</a>)</li>
            <li>
            <i>First and zeroth-order implementations of the regularized Newton method with lazy approximated Hessians.</i><br/>
            <strong>Nikita Doikov</strong> and
            <a class="dark" href="https://scholar.google.com/citations?user=PwH5lDEAAAAJ">Geovani Nunes Grapiglia</a>, 2023
            (<a href="https://arxiv.org/abs/2309.02412">arXiv</a>)</li>
            <li>
            <i>Minimizing Quasi-Self-Concordant Functions by Gradient Regularization of Newton Method.</i><br/> 
            <strong>Nikita Doikov</strong>, 2023
            (<a href="https://arxiv.org/abs/2308.14742">arXiv</a>)</li>
            <li>
            <i>Lower Complexity Bounds for Minimizing Regularized Functions.</i><br/>
            <strong>Nikita Doikov</strong>, 2022
            (<a href="https://arxiv.org/abs/2202.04545">arXiv</a>)</li>
        </ul>

        <h4>Refereed publications:</h4>
        <h4>2024</h4>
        <ul>
            <li>
            <i>Spectral Preconditioning for Gradient Methods on Graded Non-convex Functions.</i><br/>
            <strong>Nikita Doikov</strong>, 
            <a class="dark" href="https://www.sstich.ch/">Sebastian U. Stich</a>, and
            <a class="dark" href="https://people.epfl.ch/martin.jaggi">Martin Jaggi</a>, 2024
            (International Conference on Machine Learning <strong>[ICML]</strong>:
            <a href="https://proceedings.mlr.press/v235/doikov24a.html">proceedings</a>, 
            <a href="https://arxiv.org/abs/2402.04843">arXiv</a>)</li>
            <li>
            <i>On Convergence of Incremental Gradient for Non-Convex Smooth Functions.</i><br/>
            <a class="dark" href="https://koloskova.github.io/">Anastasia Koloskova</a>,
            <strong>Nikita Doikov</strong>, 
            <a class="dark" href="https://www.sstich.ch/">Sebastian U. Stich</a>, and
            <a class="dark" href="https://people.epfl.ch/martin.jaggi">Martin Jaggi</a>, 2023
            (International Conference on Machine Learning <strong>[ICML]</strong>:
            <a href="https://proceedings.mlr.press/v235/koloskova24a.html">proceedings</a>, 
            <a href="https://arxiv.org/abs/2305.19259">arXiv</a>)</li>
            <li>
            <i>Unified Convergence Theory of Stochastic and Variance-Reduced Cubic Newton Methods.</i><br/>
            <a class="dark" href="https://people.epfl.ch/el-mahdi.chayti?lang=en">El Mahdi Chayti</a>, 
            <a class="dark" href="https://people.epfl.ch/martin.jaggi">Martin Jaggi</a>, 
            and <strong>Nikita Doikov</strong>, 2023
            (Transactions on Machine Learning Research <strong>[TMLR]</strong>, <a href="https://arxiv.org/abs/2302.11962">arXiv</a>)
            </li>
            <li>
            <i>Super-Universal Regularized Newton Method.</i><br/> 
            <strong>Nikita Doikov</strong>, <a class="dark" href="https://www.konstmish.com/">Konstantin Mishchenko</a>, 
            and <a class="dark" href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>, 2022
            (SIAM Journal on Optimization <strong>[SIOPT]</strong>: <a href="https://epubs.siam.org/doi/full/10.1137/22M1519444">open access</a>, <a href="https://arxiv.org/abs/2208.05888">arXiv</a>,
            <a href="https://github.com/doikov/super-newton">code</a>)</li>
        </ul>
        <h4>2023</h4>
        <ul>
            <li>
            <i>Linearization Algorithms for Fully Composite Optimization.</i><br/>
            <a class="dark" href="https://people.epfl.ch/maria-luiza.vladarean">Maria-Luiza Vladarean</a>, 
            <strong>Nikita Doikov</strong>, 
            <a class="dark" href="https://people.epfl.ch/martin.jaggi">Martin Jaggi</a>, and 
            <a class="dark" href="https://people.epfl.ch/nicolas.flammarion">Nicolas Flammarion</a>, 2023
            (Conference on Learning Theory <strong>[COLT]</strong>: <a href="https://proceedings.mlr.press/v195/vladarean23a.html">proceedings</a>, <a href="https://arxiv.org/abs/2302.12808">arXiv</a>)
            </li>
            <li>
            <i>Polynomial Preconditioning for Gradient Methods.</i><br/>
            <strong>Nikita Doikov</strong> and <a class="dark" href="https://scholar.google.com/citations?user=u95GRZQAAAAJ&hl=en">Anton Rodomanov</a>, 2023
            (International Conference on Machine Learning <strong>[ICML]</strong>: <a href="https://proceedings.mlr.press/v202/doikov23b.html">proceedings</a>, <a href="https://arxiv.org/abs/2301.13194">arXiv</a>)</li>
            <li>
            <i>Second-order optimization with lazy Hessians.</i><br/>
            <strong>Nikita Doikov</strong>, <a class="dark" href="https://people.epfl.ch/el-mahdi.chayti">El Mahdi Chayti</a>, 
            and <a class="dark" href="https://people.epfl.ch/martin.jaggi">Martin Jaggi</a>, 2022
            (International Conference on Machine Learning <strong>[ICML]</strong> <span class="bonus">(oral presentation)</span>: <a href="https://proceedings.mlr.press/v202/doikov23a.html">proceedings</a>, <a href="https://arxiv.org/abs/2212.00781">arXiv</a>)</li>
            <li>
            <i>Gradient Regularization of Newton Method with Bregman Distances.</i><br/> 
            <strong>Nikita Doikov</strong> and <a class="dark" href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>, 2021
            (Mathematical Programming Journal <strong>[Math.Prog]</strong>: <a href="https://link.springer.com/article/10.1007/s10107-023-01943-7">open access</a>,
            <a href="https://arxiv.org/abs/2112.02952">arXiv</a>)</li>
        </ul>
        <h4>2022</h4>
        <ul>
            <li>
            <i>High-Order Optimization Methods for Fully Composite Problems.</i><br/>  
            <strong>Nikita Doikov</strong> and <a class="dark" href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>, 2021
            (SIAM Journal on Optimization <strong>[SIOPT]</strong>: <a href="https://epubs.siam.org/doi/abs/10.1137/21M1410063">open access</a>, <a href="https://arxiv.org/abs/2103.12632">arXiv</a>)</li>
            <li>
            <i>Affine-invariant contracting-point methods for Convex Optimization.</i><br/> 
            <strong>Nikita Doikov</strong> and <a class="dark" href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>, 2020
            (Mathematical Programming Journal <strong>[Math.Prog]</strong>: <a href="https://link.springer.com/article/10.1007%2Fs10107-021-01761-9">open access</a>,
            <a href="https://arxiv.org/abs/2009.08894">arXiv</a>,
            <a href="https://github.com/doikov/logsumexp-simplex">code</a>)</li>
        </ul>
        <h4>2021</h4>
        <ul>
            <li>
            <i>Local convergence of tensor methods.</i><br/> 
            <strong>Nikita Doikov</strong> and <a class="dark" href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>, 2019 
            (Mathematical Programming Journal <strong>[Math.Prog]</strong>: <a href="https://link.springer.com/article/10.1007/s10107-020-01606-x">open access</a>,
            <a href="https://arxiv.org/abs/1912.02516">arXiv</a>)</li>
            <li>
            <i>Minimizing Uniformly Convex Functions by Cubic Regularization of Newton Method.</i><br/> 
            <strong>Nikita Doikov</strong> and <a class="dark" href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>, 2019 
            (Journal of Optimization Theory and Applications <strong>[JOTA]</strong>: <a href="https://link.springer.com/article/10.1007%2Fs10957-021-01838-7">open access</a>, <a href="https://arxiv.org/abs/1905.02671">arXiv</a>)</li>
        </ul>
        <h4>2020</h4>
        <ul>
            <li>
            <i>Convex optimization based on global lower second-order models.</i><br/> 
            <strong>Nikita Doikov</strong> and <a class="dark" href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>, 2020 
            (Conference on Neural Information Processing Systems <strong>[NeurIPS]</strong> <span class="bonus">(oral presentation)</span>: <a href="https://proceedings.neurips.cc//paper_files/paper/2020/hash/c0c3a9fb8385d8e03a46adadde9af3bf-Abstract.html">proceedings</a>, 
            <a href="https://arxiv.org/abs/2006.08518">arXiv</a>,
            <a href="https://github.com/doikov/contracting-newton">code</a>)</li>            
            <li>
            <i>Stochastic Subspace Cubic Newton Method.</i><br/> 
            <a class="dark" href="https://fhanzely.github.io/">Filip Hanzely</a>, <strong>Nikita Doikov</strong>, 
            <a class="dark" href="https://richtarik.org/">Peter Richtárik</a>, 
            and <a class="dark" href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>, 2020 
            (International Conference on Machine Learning <strong>[ICML]</strong>: <a href="http://proceedings.mlr.press/v119/hanzely20a.html">proceedings</a>, 
            <a href="https://arxiv.org/abs/2002.09526">arXiv</a>)</li>
            <li>
            <i>Inexact Tensor Methods with Dynamic Accuracies.</i><br/> 
            <strong>Nikita Doikov</strong> and 
            <a class="dark" href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>, 2020 
            (International Conference on Machine Learning <strong>[ICML]</strong>: <a href="http://proceedings.mlr.press/v119/doikov20a.html">proceedings</a>, 
            <a href="https://arxiv.org/abs/2002.09403">arXiv</a>,
            <a href="https://github.com/doikov/dynamic-accuracies">code</a>)</li>
            <li>
            <i>Contracting Proximal Methods for Smooth Convex Optimization.</i><br/> 
            <strong>Nikita Doikov</strong> and <a class="dark" href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>, 
            2019 (SIAM Journal on Optimization <strong>[SIOPT]</strong>: <a href="https://epubs.siam.org/doi/abs/10.1137/19M130769X">open access</a>, <a href="https://arxiv.org/abs/1912.07972">arXiv</a>)</li>
    
        </ul>
        <h4>2018</h4>
        <ul>
            <li>
            <i>Randomized Block Cubic Newton Method.</i><br/> 
            <strong>Nikita Doikov</strong> and <a class="dark" href="https://richtarik.org/">Peter Richtárik</a>, 2018 
            (International Conference on Machine Learning <strong>[ICML]</strong> <span class="bonus">(oral presentation)</span>: <a href="http://proceedings.mlr.press/v80/doikov18a.html">proceedings</a>, <a href="https://arxiv.org/abs/1802.04084">arXiv</a>)</li>
        </ul>    
        <br/>
        <br/>
        <hr/>

        <h3 class="blue"> Recent talks </h3>
        <ul>
            <li>August 27, 2024: <strong>Spectral Preconditioning for Gradient Methods on Graded Non-convex Functions</strong>, 
                <a class="dark" href="https://sites.uclouvain.be/algopt2024/">ALGOPT</a>, Louvain-la-Neuve (<a href="slides/SpecPrecond_ALGOPT24.pdf">slides</a>) <span class="hover_img"><a href="#/">[photo&darr;]<span><img src="photos/ALGOPT24.jpg" alt="photo" height="400"/></span></a></span></li>
            <li>July 1, 2024: <strong>Minimizing quasi-self-concordant functions by gradient regularization of Newton method</strong>, EURO, Copenhagen (<a href="slides/QuasiSC_EURO24.pdf">slides</a>)</li>
            <li>June 26, 2024: <strong>Spectral Preconditioning for Gradient Methods on Graded Non-convex Functions</strong>, EUROPT, Lund (<a href="slides/SpecPrecond_EUROPT24.pdf">slides</a>)</li>
            <li>June 20, 2024: <strong>Polynomial Preconditioning for Gradient Methods</strong>, FGS-24, Gijón (<a href="slides/PolyPrecond_FGS24.pdf">slides</a>)</li>
            <li>April 9, 2024: <strong>Minimizing quasi-self-concordant functions by gradient regularization of Newton method</strong>, NOPTA, University of Antwerp (<a href="slides/QuasiSC_NOPTA24.pdf">slides</a>)</li>
            <li>August 25, 2023: <strong>Super-Universal Regularized Newton Method</strong>, EUROPT, Budapest (<a href="slides/SuperNewton_EUROPT23.pdf">slides</a>)</li>
            <li>July 20, 2023: <strong>Second-Order Optimization with Lazy Hessians</strong>, ICML, Hawaii (<a href="slides/LazyNewton_ICML23.pdf">slides</a>, <a href="slides/LazyNewton_ICML23_poster.pdf">poster</a>, <a href="https://icml.cc/virtual/2023/oral/25472">video</a>)
                <span class="hover_img"><a href="#/">[photo&darr;]<span><img src="photos/ICML23.jpg" alt="photo" height="400"/></span></a></span></li>
            <li>July 19, 2023: <strong>Polynomial Preconditioning for Gradient Methods</strong>, ICML, Hawaii (<a href="slides/PolyPrecond_ICML23_poster.pdf">poster</a>)</li>
            <li>June 3, 2023: <strong>Second-Order Optimization with Lazy Hessians</strong>,
                SIAM Conference on Optimization, Seattle (<a href="slides/LazyNewton_SIAM23.pdf">slides</a>)
                <span class="hover_img"><a href="#/">[photo&darr;]<span><img src="photos/SIAM23.jpeg" alt="photo" height="400"/></span></a></span></li>

            <li>September 27, 2022: <strong>Super-Universal Regularized Newton Method</strong>,
                <a class="dark" href="https://www.epfl.ch/labs/tml/">TML Laboratory</a>, EPFL (<a href="slides/SuperNewton_EPFL22.pdf">slides</a>)</li>

            <li>July 29, 2022: <strong>Affine-invariant contracting-point methods for Convex Optimization</strong>,
                EUROPT, Lisbon
                (<a href="slides/ContrPoint_EUROPT22.pdf">slides</a>) </li>

            <li>June 3, 2022: <strong>Second-order methods with global convergence in Convex Optimization</strong>,
                the research team of <a class="dark" href="https://www.esat.kuleuven.be/stadius/person.php?id=782">Panos Patrinos</a>, KULeuven
                (<a href="slides/SecondOrder_Leuven22.pdf">slides</a>) </li>

            <li>May 5, 2022: <strong>Optimization Methods for Fully Composite Problems</strong>,
                FGP-22, Porto 
                (<a href="slides/FullyComp_FGP22.pdf">slides</a>) </li>
            <li>February 21, 2022: <strong>Second-order methods with global convergence in Convex Optimization</strong>,
                <a class="dark" href="https://www.epfl.ch/labs/mlo/">MLO Laboratory</a>, EPFL (<a href="slides/SecondOrder_EPFL22.pdf">slides</a>)</li>
            <li>July 7, 2021: <strong>Local convergence of tensor methods</strong>,
            EUROPT, online 
                (<a href="slides/LocalTM_EUROPT21.pdf">slides</a>)</li>

            <li>March 4, 2021: <strong>Affine-invariant contracting-point methods for Convex Optimization</strong>,
            Symposium on Numerical Analysis and Optimization
            (invited by <a class="dark" href="https://scholar.google.com/citations?user=PwH5lDEAAAAJ">Geovani Nunes Grapiglia</a>), UFPR, online 
                (<a href="slides/ContrPoint_SympUFPR21.pdf">slides</a>)</li>
            <li>October 28, 2020: <strong>Convex optimization based on global lower second-order models</strong>, NeurIPS, online
                (<a href="slides/ContrNewton_NeurIPS20.pdf">slides</a>,
                 <a href="slides/ContrNewton_NeurIPS20_poster.pdf">poster</a>)</li>
            <li>June 17, 2020: <strong>Inexact Tensor Methods with Dynamic Accuracies</strong>, ICML, online
                (<a href="slides/InexactTensors_ICML20.pdf">slides</a>,
                 <a href="slides/InexactTensors_ICML20_poster.pdf">poster</a>,
                 <a href="https://slideslive.com/38928150/inexact-tensor-methods-with-dynamic-accuracies">video</a>)</li>

            <li>October 8, 2019: <strong>Proximal Method with Contractions for Smooth Convex Optimization</strong>, ICTEAM seminar, Louvain-la-Neuve</li>

            <li>September 23, 2019: <strong>Proximal Method with Contractions for Smooth Convex Optimization</strong>, 
                Optimization and Learning for Data Science seminar
                (invited by  
                <a class="dark" href="https://grishchenko.org/">Dmitry Grishchenko</a>) Université Grenoble Alpes, Grenoble
            (<a href="slides/ContrProx_Grenoble19.pdf">slides</a>)
             <span class="hover_img"><a href="#/">[photo&darr;]<span><img src="photos/Grenoble19.jpeg" alt="photo" height="400"/></span></a>
            </span>
            </li>
            
            <li>September 18, 2019: <strong>Complexity of Cubically Regularized Newton Method for Minimizing Uniformly Convex Functions</strong>, FGS-19, Nice (<a href="slides/UCNewton_FGS19.pdf">slides</a>)</li>
            
            <li>August 5, 2019: <strong>Complexity of Cubically Regularized Newton Method for Minimizing Uniformly Convex Functions</strong>, ICCOPT, Berlin</li>
            
            <li>July 5, 2019: <strong>Randomized Block Cubic Newton Method</strong>, 
            Summer School on Optimization, Big Data and Applications, Veroli
            <span class="hover_img"><a href="#/">[photo&darr;]<span><img src="photos/Veroli19.jpg" alt="photo" height="400"/></span></a></span></li>
            
            <li>June 28, 2019: <strong>Complexity of Cubically Regularized Newton Method for Minimizing Uniformly Convex Functions</strong> EUROPT, Glasgow
                <span class="hover_img"><a href="#/">[photo&darr;]<span><img src="photos/Glasgow19.jpg" alt="photo" height="400"/></span></a></span></li>

            <li>June 20, 2018: <strong>Randomized Block Cubic Newton Method</strong>, ICML, Stockholm
                (<a href="slides/RBCN_ICML18.pdf">slides</a>, 
                 <a href="slides/RBCN_ICML18_poster.pdf">poster</a>, 
                 <a href="https://www.youtube.com/watch?v=_NC9sc-nXoc&t=5051">video</a>)
             <span class="hover_img"><a href="#/">[photo&darr;]<span><img src="photos/ICML18.jpg" alt="photo" height="400"/></span></a></span></li>

            <li>June 13, 2018: <strong>Randomized Block Cubic Newton Method</strong>, 
            X Traditional summer school on Optimization, Voronovo 
            <span class="hover_img"><a href="#/">[photo&darr;]<span><img src="photos/TMSH18.jpg" alt="photo" height="400"/></span></a></span></li>

        </ul>

        <br/>
        <hr/>
        <div class="footer">
        Updated: October 31, 2024
        </div>
    </div>
</div>
</body>
</html>