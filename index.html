<html>
<head>
    <meta charset="UTF-8"/>
    <meta name="author" content="Nikita Doikov">
    <meta name="description" content="a homepage">

    <meta property="og:site_name" content="Nikita Doikov">
    <meta property="og:url" content="https://doikov.com/">
    <meta property="og:title" content="Nikita Doikov">
    <meta property="og:description" content="a homepage">
    <meta property="og:image" content="https://doikov.com/photos/me192.png">

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-170271039-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-170271039-1');
    </script>
<title>
    Nikita Doikov
</title>
    
<link rel="stylesheet" type="text/css" 
    href="https://fonts.googleapis.com/css?family=Open+Sans:400,600" />
<style type="text/css">

.page, a, h1, h3 {
    font-family: "Open Sans", sans-serif;
}

body {
    background-color: #242733;
    background-image: url("background.jpg");
    background-size: cover;
    background-repeat: no-repeat;
    margin: 0;
}

a {
    color: #9c0000;
    text-decoration: underline;
    pointer-events: auto;
}

a:focus,
a:hover {
    color: #EE0000;
    text-decoration: none;
}

.page {
    padding: 20px;
}

.sideblock {
    width: 270px;
    float: left;
    margin: 10px 20px 20px 10px;
    border: red 2px;
    background-color: #FFFFFF;
    font-family: "Open Sans";
}  

.centering {
    text-align: center;
}

.email {
    padding: 0 0 20px 20px;
}

.main {
    width: 60%;
    float: left;
    margin: 10px 10px 300px 10px;
    padding: 20px 30px 20px 30px;
    background-color: #FFFFFF;
}

.location {
    font-style: italic;
}

.footer {
    color: gray;
    font-size: 12px;
    font-style: italic;
}

.hover_img a { 
    color: #336699;
    position: relative; 
    border-bottom: 1px dotted; 
    text-decoration: none;
}

.hover_img a span { 
    position: absolute; 
    display: none; 
    z-index: 99; 
}

.hover_img a:hover {
    color: #0000EE;
}

.hover_img a:hover span { 
    display: block; 
}

.main li { padding-bottom: 17px; }

.main h4, .main h3 { color: #012042; }

</style>
</head>

<body>
<div class="page">
    <div class="sideblock">
        <img src="photos/me.jpg" alt="Me" height="270" width="270" />
        <div class="centering">
            <h1>Nikita Doikov</h1>
            <h3>a homepage</h3>
        </div>
            <ul>

                <li><a href="CV.pdf">Curriculum Vitae</a></li>

                <li><a href="https://scholar.google.com/citations?user=YNBhhjUAAAAJ&hl=en">Google Scholar</a></li>

                <li><a href="https://github.com/doikov">GitHub</a></li>

            </ul>

        <div class="email">
            <tt>Email: <br/>nikita.doikov at epfl.ch</tt>
        </div>
        
    </div>
    <div class="main">
        
        <h4>Welcome!</h4>

        I do research in Computer Science with a focus
        on Numerical Optimization and Machine Learning.

        <br/>
        <br/>

        Now I am a postdoctoral researcher at <a href="https://www.epfl.ch/">EPFL</a>, Switzerland,
        working in the <a href="https://www.epfl.ch/labs/mlo/">Machine Learning and Optimization Laboratory</a>
        with <a href="https://people.epfl.ch/martin.jaggi">Martin Jaggi</a>.

        <br/>
        <br/>
                I am excited to explore provably efficient optimization algorithms
        that exploit the problem structure and combine ideas from various fields.
        One of my areas of expertise is <i>second-order methods</i> and its <i>global complexity bounds</i>. I believe that bridging the gap between the second-order optimization theory
        and the best known computational practices is what
        can lead us to new achievements in the training process of our models.



        <br/>
        <br/>
        
        I defended my PhD in 2021 at <a href="https://uclouvain.be/en/index.html">UCLouvain</a>, Belgium, supervised by 
        <a href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>.
        My thesis is "<a href="thesis.pdf">New second-order and tensor methods
        in Convex Optimization</a>".
        
        <br/>
        <br/>
        I received a BSc degree in Computational Mathematics and Cybernetics
        from <a href="https://www.msu.ru/en/">Lomonosov Moscow State University</a> in 2015.
        I obtained a MSc degree 
        from <a href="https://www.hse.ru/en/">Higher School of Economics</a> in 2017, where I was studying advanced
        statistical and optimization methods.
        <br/>
        <br/>
        <hr/>

        <h3> Papers </h3>
        <h4> Preprints / Various</h4>
        <ul>
            <li>
            Minimizing Quasi-Self-Concordant Functions by Gradient Regularization of Newton Method, 2023
            (<a href="https://arxiv.org/abs/2308.14742">arXiv</a>)</li>
            <li>
            Shuffle SGD is Always Better than SGD: Improved Analysis of SGD with Arbitrary Data Orders, 
            with <a href="https://scholar.google.com/citations?user=ldJpvE8AAAAJ&hl=en">Anastasia Koloskova</a>, 
            <a href="https://www.sstich.ch/">Sebastian U. Stich</a>,
            and <a href="https://people.epfl.ch/martin.jaggi">Martin Jaggi</a>, 2023
            (<a href="https://arxiv.org/abs/2305.19259">arXiv</a>)</li>
            <li>
            Lower Complexity Bounds for Minimizing Regularized Functions, 2022
            (<a href="https://arxiv.org/abs/2202.04545">arXiv</a>)</li>
        </ul>

        <h4> Refereed Conference Publications </h4>
        <ul>
            <li>
            Linearization Algorithms for Fully Composite Optimization,
            with <a href="https://people.epfl.ch/maria-luiza.vladarean">Maria-Luiza Vladarean</a>, 
            <a href="https://people.epfl.ch/martin.jaggi">Martin Jaggi</a>, and 
            <a href="https://people.epfl.ch/nicolas.flammarion">Nicolas Flammarion</a>, 2023
            (COLT, <a href="https://arxiv.org/abs/2302.12808">arXiv</a>)
            </li>
            <li>
            Polynomial Preconditioning for Gradient Methods,
            with <a href="https://scholar.google.com/citations?user=u95GRZQAAAAJ&hl=en">Anton Rodomanov</a>, 2023
            (ICML, <a href="https://arxiv.org/abs/2301.13194">arXiv</a>)
            </li>
            <li>
            Second-order optimization with lazy Hessians,
            with <a href="https://people.epfl.ch/el-mahdi.chayti">El Mahdi Chayti</a> 
            and <a href="https://people.epfl.ch/martin.jaggi">Martin Jaggi</a>, 2023
            (ICML, <a href="https://arxiv.org/abs/2212.00781">arXiv</a>)
            </li>
            <li>
            Convex optimization based on global lower second-order models, 
            with <a href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>, 2020 (NeurIPS <a href="https://proceedings.neurips.cc//paper_files/paper/2020/hash/c0c3a9fb8385d8e03a46adadde9af3bf-Abstract.html">proceedings</a>, 
            <a href="https://arxiv.org/abs/2006.08518">arXiv</a>,
            <a href="https://github.com/doikov/contracting-newton">code</a>)</li>            
            <li>
            Stochastic Subspace Cubic Newton Method, 
            with <a href="https://fhanzely.github.io/">Filip Hanzely</a>, 
            <a href="https://richtarik.org/">Peter Richtárik</a>, 
            and Yurii Nesterov, 2020 (ICML <a href="http://proceedings.mlr.press/v119/hanzely20a.html">proceedings</a>, 
            <a href="https://arxiv.org/abs/2002.09526">arXiv</a>)</li>
            <li>
            Inexact Tensor Methods with Dynamic Accuracies, 
            with <a href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>, 2020 (ICML <a href="http://proceedings.mlr.press/v119/doikov20a.html">proceedings</a>, 
            <a href="https://arxiv.org/abs/2002.09403">arXiv</a>,
            <a href="https://github.com/doikov/dynamic-accuracies">code</a>)</li>
            <li>
            Randomized Block Cubic Newton Method, 
            with <a href="https://richtarik.org/">Peter Richtárik</a>, 2018 
            (ICML <a href="http://proceedings.mlr.press/v80/doikov18a.html">proceedings</a>, <a href="https://arxiv.org/abs/1802.04084">arXiv</a>)</li>
        </ul>

        <h4> Journal Publications </h4>
        <ul>
            <li>
            Super-Universal Regularized Newton Method, 
            with <a href="https://www.konstmish.com/">Konstantin Mishchenko</a> 
            and <a href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>, 2023
            (to appear in SIOPT journal, <a href="https://arxiv.org/abs/2208.05888">arXiv</a>,
             <a href="https://github.com/doikov/super-newton">code</a>)</li>
            <li>
            Gradient Regularization of Newton Method with Bregman Distances, with <a href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>, 2023
            (Math. Program. <a href="https://link.springer.com/article/10.1007/s10107-023-01943-7">journal</a>,
            <a href="https://arxiv.org/abs/2112.02952">arXiv</a>)</li>
            <li>
            High-Order Optimization Methods for Fully Composite Problems, with <a href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>, 2022
            (SIOPT <a href="https://epubs.siam.org/doi/abs/10.1137/21M1410063">journal</a>, <a href="https://arxiv.org/abs/2103.12632">arXiv</a>)</li>
            <li>
            Affine-invariant contracting-point methods for Convex Optimization, 
            with <a href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>, 2022 (Math. Program. <a href="https://link.springer.com/article/10.1007%2Fs10107-021-01761-9">journal</a>,
            <a href="https://arxiv.org/abs/2009.08894">arXiv</a>,
            <a href="https://github.com/doikov/logsumexp-simplex">code</a>)</li>
            <li>
            Local convergence of tensor methods,
            with <a href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>, 2021 (Math. Program. <a href="https://link.springer.com/article/10.1007/s10107-020-01606-x">journal</a>,
            <a href="https://arxiv.org/abs/1912.02516">arXiv</a>)</li>
            <li>
            Minimizing Uniformly Convex Functions by Cubic Regularization of Newton Method,
            with <a href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>, 2021 (JOTA <a href="https://link.springer.com/article/10.1007%2Fs10957-021-01838-7">journal</a>, <a href="https://arxiv.org/abs/1905.02671">arXiv</a>)</li>
            <li>
            Contracting Proximal Methods for Smooth Convex Optimization, 
            with <a href="https://scholar.google.com/citations?user=DJ8Ep8YAAAAJ&hl=en">Yurii Nesterov</a>, 2020 (SIOPT <a href="https://epubs.siam.org/doi/abs/10.1137/19M130769X">journal</a>, <a href="https://arxiv.org/abs/1912.07972">arXiv</a>)</li>
        </ul>

        <br/>
        <hr/>

        <h3 class="blue"> Recent Talks </h3>
        <ul>
            <li>August 25, 2023: <strong>Super-Universal Regularized Newton Method</strong>, EUROPT, Budapest (<a href="slides/SuperNewton_EUROPT23.pdf">slides</a>)</li>
            <li>July 20, 2023: <strong>Second-Order Optimization with Lazy Hessians</strong>, ICML, Hawaii (<a href="slides/LazyNewton_ICML23.pdf">slides</a>, <a href="slides/LazyNewton_ICML23_poster.pdf">poster</a>)
                <span class="hover_img"><a href="#/">[photo&darr;]<span><img src="photos/ICML23.jpg" alt="photo" height="400"/></span></a></span></li>
            <li>July 19, 2023: <strong>Polynomial Preconditioning for Gradient Methods</strong>, ICML, Hawaii (<a href="slides/PolyPrecond_ICML23_poster.pdf">poster</a>)</li>
            <li>June 3, 2023: <strong>Second-Order Optimization with Lazy Hessians</strong>,
                SIAM Conference on Optimization, Seattle (<a href="slides/LazyNewton_SIAM23.pdf">slides</a>)
                <span class="hover_img"><a href="#/">[photo&darr;]<span><img src="photos/SIAM23.jpeg" alt="photo" height="400"/></span></a></span></li>

            <li>September 27, 2022: <strong>Super-Universal Regularized Newton Method</strong>,
                TML Laboratory, EPFL (<a href="slides/SuperNewton_EPFL22.pdf">slides</a>)</li>

            <li>July 29, 2022: <strong>Affine-invariant contracting-point methods for Convex Optimization</strong>,
                EUROPT, Lisbon
                (<a href="slides/ContrPoint_EUROPT22.pdf">slides</a>) </li>

            <li>June 3, 2022: <strong>Second-order methods with global convergence in Convex Optimization</strong>,
                the research team of <a href="https://www.esat.kuleuven.be/stadius/person.php?id=782">Panos Patrinos</a>, KULeuven
                (<a href="slides/SecondOrder_Leuven22.pdf">slides</a>) </li>

            <li>May 5, 2022: <strong>Optimization Methods for Fully Composite Problems</strong>,
                FGP-22, Porto 
                (<a href="slides/FullyComp_FGP22.pdf">slides</a>) </li>
            <li>February 21, 2022: <strong>Second-order methods with global convergence in Convex Optimization</strong>,
                MLO Laboratory, EPFL (<a href="slides/SecondOrder_EPFL22.pdf">slides</a>)</li>
            <li>July 7, 2021: <strong>Local convergence of tensor methods</strong>,
            EUROPT, online 
                (<a href="slides/LocalTM_EUROPT21.pdf">slides</a>)</li>

            <li>March 4, 2021: <strong>Affine-invariant contracting-point methods for Convex Optimization</strong>,
            Symposium on Numerical Analysis and Optimization
            (invited by <a href="https://scholar.google.com/citations?user=PwH5lDEAAAAJ">Geovani Grapiglia</a>), UFPR, online 
                (<a href="slides/ContrPoint_SympUFPR21.pdf">slides</a>)</li>
            <li>October 28, 2020: <strong>Convex optimization based on global lower second-order models</strong>, NeurIPS, online
                (<a href="slides/ContrNewton_NeurIPS20.pdf">slides</a>,
                 <a href="slides/ContrNewton_NeurIPS20_poster.pdf">poster</a>)</li>
            <li>June 17, 2020: <strong>Inexact Tensor Methods with Dynamic Accuracies</strong>, ICML, online
                (<a href="slides/InexactTensors_ICML20.pdf">slides</a>,
                 <a href="slides/InexactTensors_ICML20_poster.pdf">poster</a>,
                 <a href="https://slideslive.com/38928150/inexact-tensor-methods-with-dynamic-accuracies">video</a>)</li>

            <li>October 8, 2019: <strong>Proximal Method with Contractions for Smooth Convex Optimization</strong>, ICTEAM seminar, Louvain-la-Neuve</li>

            <li>September 23, 2019: <strong>Proximal Method with Contractions for Smooth Convex Optimization</strong>, 
                Optimization and Learning for Data Science seminar
                (invited by  
                <a href="https://grishchenko.org/">Dmitry Grishchenko</a>) Université Grenoble Alpes, Grenoble
            (<a href="slides/ContrProx_Grenoble19.pdf">slides</a>)
             <span class="hover_img"><a href="#/">[photo&darr;]<span><img src="photos/Grenoble19.jpeg" alt="photo" height="400"/></span></a>
            </span>
            </li>
            
            <li>September 18, 2019: <strong>Complexity of Cubically Regularized Newton Method for Minimizing Uniformly Convex Functions</strong>, FGS-19, Nice (<a href="slides/UCNewton_FGS19.pdf">slides</a>)</li>
            
            <li>August 5, 2019: <strong>Complexity of Cubically Regularized Newton Method for Minimizing Uniformly Convex Functions</strong>, ICCOPT, Berlin</li>
            
            <li>July 5, 2019: <strong>Randomized Block Cubic Newton Method</strong>, 
            Summer School on Optimization, Big Data and Applications, Veroli
            <span class="hover_img"><a href="#/">[photo&darr;]<span><img src="photos/Veroli19.jpg" alt="photo" height="400"/></span></a></span></li>
            
            <li>June 28, 2019: <strong>Complexity of Cubically Regularized Newton Method for Minimizing Uniformly Convex Functions</strong> EUROPT, Glasgow
                <span class="hover_img"><a href="#/">[photo&darr;]<span><img src="photos/Glasgow19.jpg" alt="photo" height="400"/></span></a></span></li>

            <li>June 20, 2018: <strong>Randomized Block Cubic Newton Method</strong>, ICML, Stockholm
                (<a href="slides/RBCN_ICML18.pdf">slides</a>, 
                 <a href="slides/RBCN_ICML18_poster.pdf">poster</a>, 
                 <a href="https://www.youtube.com/watch?v=_NC9sc-nXoc&t=5051">video</a>)
             <span class="hover_img"><a href="#/">[photo&darr;]<span><img src="photos/ICML18.jpg" alt="photo" height="400"/></span></a></span></li>

            <li>June 13, 2018: <strong>Randomized Block Cubic Newton Method</strong>, 
            X Traditional summer school on Optimization, Voronovo 
            <span class="hover_img"><a href="#/">[photo&darr;]<span><img src="photos/TMSH18.jpg" alt="photo" height="400"/></span></a></span></li>

        </ul>

        <br/>
        <hr/>
        <div class="footer">
        Updated: August 29, 2023
        </div>
    </div>
</div>
</body>
</html>